import os
import torch
import torch.nn as nn
from PIL import Image
import torchvision.transforms as transforms
import torchvision.models as models

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

embed_size = 256
hidden_size = 256
num_layers = 1
weights_path = "model_weights.pth"
vocab_path = "vocab.txt"

with open(vocab_path, "r", encoding="utf-8") as f:
    vocab = [line.strip() for line in f.readlines()]
word_to_idx = {w: i for i, w in enumerate(vocab)}
idx_to_word = {i: w for w, i in word_to_idx.items()}
vocab_size = len(vocab)

class ResNetEncoder(nn.Module):
    def __init__(self, output_size):
        super().__init__()
        resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
        for p in resnet.parameters():
            p.requires_grad_(False)
        num_ftrs = resnet.fc.in_features
        resnet.fc = nn.Linear(num_ftrs, output_size)
        self.resnet = resnet
        self.bn = nn.BatchNorm1d(output_size)

    def forward(self, images):
        return self.bn(self.resnet(images))

class LSTMDecoder(nn.Module):
    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):
        super().__init__()
        self.num_layers = num_layers
        self.hidden_size = hidden_size
        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)
        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)
        self.fc_out = nn.Linear(hidden_size, vocab_size)

    def forward_step(self, x, hidden):
        emb = self.embedding(x)
        out, hidden = self.lstm(emb, hidden)
        out = self.fc_out(out.squeeze(1))
        return out, hidden

class EncoderDecoderModel(nn.Module):
    def __init__(self, encoder, decoder):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder

encoder = ResNetEncoder(hidden_size).to(device)
decoder = LSTMDecoder(vocab_size, embed_size, hidden_size, num_layers).to(device)
model = EncoderDecoderModel(encoder, decoder)
model.load_state_dict(torch.load(weights_path, map_location=device))
model.eval()

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

def generate_caption(image_path, max_len=30):
    image = transform(Image.open(image_path).convert("RGB")).unsqueeze(0).to(device)
    with torch.no_grad():
        features = model.encoder(image)
        inputs = torch.tensor([[word_to_idx["<sos>"]]], dtype=torch.long).to(device)
        hidden = (features.unsqueeze(0).repeat(num_layers, 1, 1),
                  features.unsqueeze(0).repeat(num_layers, 1, 1))
        caption = []
        for _ in range(max_len):
            out, hidden = model.decoder.forward_step(inputs, hidden)
            _, predicted = out.max(1)
            word = idx_to_word[predicted.item()]
            if word == "<eos>":
                break
            caption.append(word)
            inputs = predicted.unsqueeze(1)
    return " ".join(caption)

if __name__ == "__main__":
    test_image = "data/traintestfolder/train/images/sample.jpg"
    caption = generate_caption(test_image)
    print("Generated Caption:", caption)
